{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11977549,"sourceType":"datasetVersion","datasetId":7532413}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Automatic Speech Recognition (ASR) has leapt from research labs onto every phone and smart-speaker we own, yet the recipe that turns a WAV file into words is still a bit of a black box for many data-scientists. This notebook lifts the lid by guiding you, code cell by code cell, through the entire pipeline needed to build a character-level speech recogniser using nothing more than open-source PyTorch, torchaudio and a GPU.\n\nBy the end you’ll have a working speech recogniser trained from scratch, a solid grasp of each design decision, and ready-to-reuse code snippets for your own audio projects.","metadata":{}},{"cell_type":"code","source":"import torch, torchaudio, random, re\nfrom torch import nn\nfrom itertools import groupby\nfrom pathlib import Path\nfrom jiwer import wer, cer\n\nSAMPLE_RATE = 16_000       \nBLANK_TOKEN = \"<BLANK>\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:39:35.682546Z","iopub.execute_input":"2025-05-28T01:39:35.682813Z","iopub.status.idle":"2025-05-28T01:39:35.687579Z","shell.execute_reply.started":"2025-05-28T01:39:35.682793Z","shell.execute_reply":"2025-05-28T01:39:35.686937Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Dataset\n\n* train-clean-100: small enough that you can train a character-level model in a couple of hours on a single GPU, yet large enough to reach ~20 % WER with a modest network.\n\n* test-clean: the standard benchmark used in almost every ASR paper; you can directly compare your WER/CER to published numbers.","metadata":{}},{"cell_type":"code","source":"train_ds = torchaudio.datasets.LIBRISPEECH(\".\", url=\"train-clean-100\", download=True)\ntest_ds  = torchaudio.datasets.LIBRISPEECH(\".\", url=\"test-clean\",  download=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:15:50.424623Z","iopub.execute_input":"2025-05-27T20:15:50.425196Z","iopub.status.idle":"2025-05-27T20:20:26.310033Z","shell.execute_reply.started":"2025-05-27T20:15:50.425172Z","shell.execute_reply":"2025-05-27T20:20:26.309242Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5.95G/5.95G [03:42<00:00, 28.7MB/s] \n100%|██████████| 331M/331M [00:13<00:00, 25.3MB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Tokenizer\n\nBefore the acoustic model can learn anything, we must convert text transcripts into integer sequences that a loss function (CTC in our case) can compare to the model’s output. This cell builds the character-level vocabulary (\"tokenizer\") from the training set and supplies two helper functions:\n\n* encode() – turns a raw transcript string into a 1-D torch.Tensor of token IDs.\n\n* decode() – performs the inverse: given a list/array of IDs it reconstructs the text.\n\nBecause CTC needs a special blank token to indicate “no character emitted”, we place BLANK_TOKEN at index 0. Characters are then assigned indices 1 … N in deterministic (sorted) order so that training and inference share exactly the same mapping.","metadata":{}},{"cell_type":"code","source":"def yield_text(data_set):\n    \"\"\"\n    Generator that iterates over the dataset and yields each transcript\n    as a *list of individual characters* (lower-cased).\n    \n    We ignore all the other items in the dataset tuple (waveform, rate, etc.)\n    by unpacking them into underscores.\n    \"\"\"\n    for _, _, txt, _, _, _ in data_set: # (wave, sr, text, speaker_id, chapter_id, utt_id)\n        yield list(txt.lower()) # lower-case for case-insensitive training\n\n# Build character vocabulary\n#  • BLANK_TOKEN goes at index 0 for CTC\n#  • sorted() gives deterministic order (important when you resume training)\nvocab = [BLANK_TOKEN] + sorted(\n            set(ch                           # unique characters …\n                for txt in yield_text(train_ds)   # … across every transcript\n                for ch in txt)\n        )\n\n# stoi = “string-to-index\" lookup: character -> integer ID\nstoi  = {ch: i for i, ch in enumerate(vocab)}\n\ndef encode(text: str) -> torch.Tensor:\n    \"\"\"\n    Map a string to a 1-D tensor of integer IDs (dtype long).\n    \"\"\"\n    return torch.tensor([stoi[c] for c in text.lower()], dtype=torch.long)\n\ndef decode(ids) -> str:\n    \"\"\"\n    Map a sequence (list / tensor) of IDs back to a string.\n    \"\"\"\n    return \"\".join(vocab[i] for i in ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:22:23.525020Z","iopub.execute_input":"2025-05-27T20:22:23.525282Z","iopub.status.idle":"2025-05-27T20:24:25.835246Z","shell.execute_reply.started":"2025-05-27T20:22:23.525264Z","shell.execute_reply":"2025-05-27T20:24:25.834456Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Audio Transform\n\nNeural ASR models don’t work directly on raw 16 kHz audio; they expect 2-D time × frequency representations that capture the perceptually important parts of speech.\nThe industry-standard front-end is a log-Mel spectrogram:\n\n1. Short-time Fourier Transform (STFT) chops the waveform into overlapping 25 ms windows (400 samples at 16 kHz) and converts them to frequency bins.\n   \n2. A Mel filterbank (80 filters here) compresses those bins onto a scale that mimics human pitch perception.\n\n3. We take the logarithm (dB) so differences in loudness become additive.\n\nDuring training we additionally apply SpecAugment, masking random time and frequency stripes, to make the model robust to speaker and noise variation ","metadata":{}},{"cell_type":"code","source":"mel = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,   # target 16 kHz\n        n_fft        = 400,        # 25 ms analysis window (400 / 16 kHz)\n        hop_length   = 160,        # 10 ms stride  (overlap = 15 ms)\n        n_mels       = 80)         # # of Mel bands -> feature dim F = 80\n\ndb  = torchaudio.transforms.AmplitudeToDB()   # converts power -> log-dB\n\ndef spec_augment(spec):\n    \"\"\"\n    Light variant of SpecAugment:\n    • FrequencyMasking    hides  up to 15 Mel bands   (simulates channel noise)\n    • TimeMasking         hides  up to 35 frames      (simulates drop-outs)\n    \"\"\"\n    spec = torchaudio.transforms.FrequencyMasking(freq_mask_param=15)(spec)\n    return torchaudio.transforms.TimeMasking(time_mask_param=35)(spec)\n\ndef preprocess(wav, sr, augment=False):\n    \"\"\"\n    wav      : Tensor (1, N)   – mono waveform\n    sr       : int             – original sample-rate\n    augment  : bool            – enable SpecAugment (use True for training)\n    \n    Returns: Tensor (T, F)     – log-Mel spectrogram\n    \"\"\"\n    # Resample if the incoming clip is not already 16 kHz\n    if sr != SAMPLE_RATE:\n        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n    \n    # STFT -> Mel filterbank -> log-dB  (shape: 1 × F × T)\n    spec = db(mel(wav))\n    \n    # Optional data-augmentation\n    if augment:\n        spec = spec_augment(spec)\n    \n    # Tidy up tensor shape:   (1, F, T) -> (T, F)\n    return spec.squeeze(0).transpose(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:24:49.784503Z","iopub.execute_input":"2025-05-27T20:24:49.785239Z","iopub.status.idle":"2025-05-27T20:24:49.970250Z","shell.execute_reply.started":"2025-05-27T20:24:49.785215Z","shell.execute_reply":"2025-05-27T20:24:49.969409Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Model\n\nAutomatic speech-recognition models need to:\n\n1. Condense local time-frequency patterns (formants, plosives, ­s-bursts…) that appear in the log-Mel spectrogram.\n\n2. Model long-range context so the network “remembers” what letters have already been emitted.\n\n3. Project the encoded sequence onto the vocabulary that the CTC loss expects (blank + characters).\n\nA Convolution + Recurrent + feed-forward (CRNN) stack is a light-weight way to do all three:\n\n* Small 2D CNN front-end learns local spectral filters and reduces frequency variance.\n\n* Bidirectional LSTM encoder captures left-hand and right-hand context over hundreds of frames.\n\n* Linear head maps the hidden representation to per-time-step log-probabilities over the vocabulary.","metadata":{}},{"cell_type":"code","source":"class CRNN(nn.Module):\n\n    def __init__(self, vocab_size,\n                 feat_dim=80,          # F  – Mel bands coming from preprocess()\n                 hidden=384,           # H  – LSTM hidden size\n                 layers=5,             # #  – stacked BiLSTM layers\n                 dropout=0.1):\n        super().__init__()\n\n        # ---------- Convolutional front-end --------------------------\n        # Two 3×3 conv layers (stride 1) act on the spectrogram as if it\n        # were a single-channel image:  (B, 1, T, F) -> (B, 32, T, F)\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1), nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=(3, 3), padding=1), nn.ReLU()\n        )\n        \n        # ---------- Recurrent encoder (bidirectional) ---------------\n        # Input size to the LSTM is 32 * feat_dim because the conv keeps\n        # 32 channels and we later flatten the frequency axis.\n        self.rnn = nn.LSTM(\n            input_size   = feat_dim * 32,  # 32×80 = 2560\n            hidden_size  = hidden,         # 384\n            num_layers   = layers,         # 5 stacked layers\n            batch_first  = True,           # (B, T, ·)\n            bidirectional=True,\n            dropout      = dropout\n        )\n\n        # ---------- Classification head -----------------------------\n        # Two-layer MLP with GELU + LayerNorm stabilises training before\n        # projecting to vocab_size (includes the CTC blank token).\n        self.fc = nn.Sequential(\n            nn.Linear(hidden * 2, hidden),   # ×2 for bidirection\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Linear(hidden, vocab_size)    # logits per character\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: Tensor (B, T, F) – batch of log-Mel spectrograms\n        Returns: log-softmax probabilities  (B, T, vocab)\n        \"\"\"\n        # 1. CNN expects channel dim -> add unsqueeze(1) to get (B, 1, T, F)\n        x = self.cnn(x.unsqueeze(1))                 # (B, 32, T, F)\n\n        # 2. Flatten CNN output so each time-step is a 1-D vector\n        B, C, T, F = x.shape                         # C = 32\n        x = x.permute(0, 2, 1, 3).reshape(B, T, C * F)  # (B, T, 32·F)\n\n        # 3. Bidirectional LSTM encoder\n        x, _ = self.rnn(x)                           # (B, T, 2H)\n\n        # 4. MLP head -> character logits, then log-softmax for CTC loss\n        return self.fc(x).log_softmax(dim=-1)        # (B, T, vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:25:13.964731Z","iopub.execute_input":"2025-05-27T20:25:13.965046Z","iopub.status.idle":"2025-05-27T20:25:13.971382Z","shell.execute_reply.started":"2025-05-27T20:25:13.965024Z","shell.execute_reply":"2025-05-27T20:25:13.970700Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Data Loader\n\nPyTorch’s DataLoader batches together whatever a Dataset yields. ASR datasets, however, contain variable-length utterances:\n\n* Spectrograms differ in time steps T.\n\n* Transcripts differ in number of characters.\n\nThe model still wants tensors of equal shape in a batch, and CTC needs the true lengths of every sequence to ignore the right-hand padding. Therefore we write a small collate function that:\n\n1. Converts each (waveform, ..., text) sample -> (spec, label).\n\n2. Records the real input and target lengths.\n\n3. Pads spectrograms so they stack into a single tensor.\n\nDuring training we also enable SpecAugment inside the same function so that augmentation is applied after the audio is loaded but before the model sees it.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate(batch, augment=False):\n    \"\"\"\n    batch  : list[ tuple ]  – items coming from the Dataset\n    augment: bool           – if True, apply SpecAugment\n    \n    Returns\n    -------\n    specs        : Tensor (B, T_max, F)      – zero-padded log-Mel batches\n    labels       : 1-D Tensor (sum targets)  – all character IDs concatenated\n    input_lens   : list[int]                 – true T for each utterance\n    target_lens  : list[int]                 – number of chars per transcript\n    \"\"\"\n    specs, labels, input_lens, target_lens = [], [], [], []\n\n    # unpack the dataset tuples\n    for wav, sr, txt, *_ in batch:          # we ignore the speaker/chapter IDs\n        spec  = preprocess(wav, sr, augment)   # (T, F)\n        specs.append(spec)\n        \n        lab   = encode(txt)                   # 1-D tensor of char IDs\n        labels.append(lab)\n        \n        input_lens.append(spec.shape[0])      # T  (time frames)\n        target_lens.append(len(lab))          # #chars\n\n    # ---------- pad the variable-length spectrograms -------------------\n    # pad_sequence -> (B, T_max, F), filled with zeros on the right\n    specs  = nn.utils.rnn.pad_sequence(specs, batch_first=True).float()\n    \n    # ---------- flatten the list of 1-D label tensors ------------------\n    labels = torch.cat(labels)               # CTC expects 1-D concat\n    \n    return specs, labels, input_lens, target_lens\n\ntrain_loader = DataLoader(train_ds, 32, shuffle=True, collate_fn=lambda b: collate(b, True))\ntest_loader  = DataLoader(test_ds,  8, shuffle=False, collate_fn=collate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:50:21.892699Z","iopub.execute_input":"2025-05-27T20:50:21.893217Z","iopub.status.idle":"2025-05-27T20:50:21.899032Z","shell.execute_reply.started":"2025-05-27T20:50:21.893194Z","shell.execute_reply":"2025-05-27T20:50:21.898272Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Training\n\nNow we are ready to Instantiate the network with the right vocabulary size, detect whether the notebook is running on a GPU (or several). If multiple GPUs are available, wrap the model in nn.DataParallel so the same forward/back-prop step is executed on every GPU and the gradients are automatically averaged. Move the (possibly wrapped) model onto the chosen device so all its parameters and buffers live in GPU memory (or CPU if no GPU).","metadata":{}},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nn_gpus  = torch.cuda.device_count()\nmodel  = CRNN(len(vocab))\nif n_gpus > 1:\n    print(f\"Using {n_gpus} GPUs via DataParallel\")\n    model = nn.DataParallel(model)  # replicates model each forward\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T03:25:50.472798Z","iopub.execute_input":"2025-05-28T03:25:50.473516Z","iopub.status.idle":"2025-05-28T03:25:50.477838Z","shell.execute_reply.started":"2025-05-28T03:25:50.473492Z","shell.execute_reply":"2025-05-28T03:25:50.477146Z"}},"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): CRNN(\n    (cnn): Sequential(\n      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n    )\n    (rnn): LSTM(2560, 384, num_layers=5, batch_first=True, dropout=0.1, bidirectional=True)\n    (fc): Sequential(\n      (0): Linear(in_features=768, out_features=384, bias=True)\n      (1): GELU(approximate='none')\n      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (3): Linear(in_features=384, out_features=29, bias=True)\n    )\n  )\n)\nTotal parameters: 23547261\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"from torch.amp import GradScaler, autocast\n\nctc    = nn.CTCLoss(blank=0, zero_infinity=True) # alignment-free loss\nopt    = torch.optim.AdamW(model.parameters(), 5e-4, weight_decay=1e-4) # Adam + decoupled weight-decay\n# One-cycle policy: large LR ▸ small LR over 1 training “cycle” (here 20 epochs)\nsched  = torch.optim.lr_scheduler.OneCycleLR(opt, 5e-4, steps_per_epoch=len(train_loader), epochs=20)\nscaler = GradScaler() # dynamic loss-scaling for FP16\n\nbest_wer, patience, PATIENCE = 1.0, 0, 4   # early stop if WER doesn’t improve 4 evals\n\nfor epoch in range(20):\n    \n    model.train() # enable dropout, etc.\n    \n    for step, (x, y, in_len, tar_len) in enumerate(train_loader):\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad()\n\n        # Automatic Mixed Precision: FP16 forward & backward,\n        # FP32 master weights held by optimiser.\n        with autocast(device_type=device):\n            logprobs = model(x)                     # (B, T, V)\n            loss = ctc(logprobs.permute(1,0,2), y, in_len, tar_len) # CTC expects (T, B, V)\n\n        # scale -> backward (prevent underflow), then unscale for safe clipping\n        scaler.scale(loss).backward()\n        scaler.unscale_(opt)\n        nn.utils.clip_grad_norm_(model.parameters(), 5.0) # exploding grads\n        \n        scaler.step(opt) # FP32 weight update\n        scaler.update() # adjust scaling factor for next iter\n        sched.step() # advance LR schedule\n        \n        if step % 100 == 0:\n            print(f\"[{epoch}] step {step}   loss {loss.item():.3f}\")\n\n    # --- validation & early-stopping\n    model.eval(); hyp, ref = [], []\n    with torch.no_grad():\n        for x, y, in_len, tar_len in test_loader:\n            x = x.to(device)\n            lp = model(x)                    # (B, T, V)\n            ids = lp.argmax(-1)              # greedy decode\n            for i, l in enumerate(in_len):\n                hyp.append(decode([k for k,_ in groupby(ids[i][:l].cpu().tolist()) if k!=0]))\n            split = torch.split(y, tar_len)\n            ref.extend([decode(t.tolist()) for t in split])\n    val_wer = wer(ref, hyp)\n    print(f\"Epoch {epoch}  valid WER: {val_wer:.3%}\")\n    if val_wer + 0.002 < best_wer:\n        best_wer, patience = val_wer, 0\n        torch.save(model.state_dict(), \"best_asr.pt\")\n    else:\n        patience += 1\n        if patience >= PATIENCE:\n            print(\"Early stopping – no improvement\"); break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T20:55:45.494760Z","iopub.execute_input":"2025-05-27T20:55:45.495088Z","iopub.status.idle":"2025-05-28T01:38:53.537694Z","shell.execute_reply.started":"2025-05-27T20:55:45.495068Z","shell.execute_reply":"2025-05-28T01:38:53.536859Z"}},"outputs":[{"name":"stdout","text":"cuda:0\nUsing 2 GPUs via DataParallel\n[0] step 0   loss 18.155\n[0] step 100   loss 6.043\n[0] step 200   loss 5.074\n[0] step 300   loss 3.448\n[0] step 400   loss 2.872\n[0] step 500   loss 2.840\n[0] step 600   loss 2.749\n[0] step 700   loss 2.740\n[0] step 800   loss 2.672\nEpoch 0  valid WER: 98.741%\n[1] step 0   loss 2.681\n[1] step 100   loss 2.661\n[1] step 200   loss 2.536\n[1] step 300   loss 2.259\n[1] step 400   loss 1.996\n[1] step 500   loss 1.850\n[1] step 600   loss 1.693\n[1] step 700   loss 1.500\n[1] step 800   loss 1.299\nEpoch 1  valid WER: 87.403%\n[2] step 0   loss 1.323\n[2] step 100   loss 1.219\n[2] step 200   loss 1.192\n[2] step 300   loss 1.075\n[2] step 400   loss 1.030\n[2] step 500   loss 1.005\n[2] step 600   loss 0.913\n[2] step 700   loss 0.923\n[2] step 800   loss 0.852\nEpoch 2  valid WER: 67.565%\n[3] step 0   loss 0.797\n[3] step 100   loss 0.752\n[3] step 200   loss 0.790\n[3] step 300   loss 0.752\n[3] step 400   loss 0.681\n[3] step 500   loss 0.778\n[3] step 600   loss 0.760\n[3] step 700   loss 0.623\n[3] step 800   loss 0.668\nEpoch 3  valid WER: 54.004%\n[4] step 0   loss 0.599\n[4] step 100   loss 0.591\n[4] step 200   loss 0.600\n[4] step 300   loss 0.597\n[4] step 400   loss 0.569\n[4] step 500   loss 0.577\n[4] step 600   loss 0.543\n[4] step 700   loss 0.524\n[4] step 800   loss 0.537\nEpoch 4  valid WER: 44.585%\n[5] step 0   loss 0.538\n[5] step 100   loss 0.498\n[5] step 200   loss 0.429\n[5] step 300   loss 0.448\n[5] step 400   loss 0.496\n[5] step 500   loss 0.481\n[5] step 600   loss 0.511\n[5] step 700   loss 0.531\n[5] step 800   loss 0.484\nEpoch 5  valid WER: 39.075%\n[6] step 0   loss 0.438\n[6] step 100   loss 0.373\n[6] step 200   loss 0.405\n[6] step 300   loss 0.400\n[6] step 400   loss 0.460\n[6] step 500   loss 0.339\n[6] step 600   loss 0.371\n[6] step 700   loss 0.388\n[6] step 800   loss 0.362\nEpoch 6  valid WER: 35.094%\n[7] step 0   loss 0.347\n[7] step 100   loss 0.338\n[7] step 200   loss 0.296\n[7] step 300   loss 0.378\n[7] step 400   loss 0.337\n[7] step 500   loss 0.345\n[7] step 600   loss 0.277\n[7] step 700   loss 0.349\n[7] step 800   loss 0.345\nEpoch 7  valid WER: 32.667%\n[8] step 0   loss 0.300\n[8] step 100   loss 0.311\n[8] step 200   loss 0.284\n[8] step 300   loss 0.299\n[8] step 400   loss 0.324\n[8] step 500   loss 0.324\n[8] step 600   loss 0.273\n[8] step 700   loss 0.332\n[8] step 800   loss 0.273\nEpoch 8  valid WER: 29.660%\n[9] step 0   loss 0.232\n[9] step 100   loss 0.289\n[9] step 200   loss 0.252\n[9] step 300   loss 0.222\n[9] step 400   loss 0.220\n[9] step 500   loss 0.259\n[9] step 600   loss 0.243\n[9] step 700   loss 0.254\n[9] step 800   loss 0.239\nEpoch 9  valid WER: 27.610%\n[10] step 0   loss 0.263\n[10] step 100   loss 0.195\n[10] step 200   loss 0.243\n[10] step 300   loss 0.187\n[10] step 400   loss 0.244\n[10] step 500   loss 0.233\n[10] step 600   loss 0.205\n[10] step 700   loss 0.230\n[10] step 800   loss 0.228\nEpoch 10  valid WER: 26.210%\n[11] step 0   loss 0.174\n[11] step 100   loss 0.211\n[11] step 200   loss 0.189\n[11] step 300   loss 0.176\n[11] step 400   loss 0.188\n[11] step 500   loss 0.181\n[11] step 600   loss 0.191\n[11] step 700   loss 0.197\n[11] step 800   loss 0.208\nEpoch 11  valid WER: 24.625%\n[12] step 0   loss 0.183\n[12] step 100   loss 0.193\n[12] step 200   loss 0.195\n[12] step 300   loss 0.196\n[12] step 400   loss 0.195\n[12] step 500   loss 0.184\n[12] step 600   loss 0.150\n[12] step 700   loss 0.172\n[12] step 800   loss 0.171\nEpoch 12  valid WER: 23.982%\n[13] step 0   loss 0.169\n[13] step 100   loss 0.141\n[13] step 200   loss 0.161\n[13] step 300   loss 0.179\n[13] step 400   loss 0.187\n[13] step 500   loss 0.175\n[13] step 600   loss 0.158\n[13] step 700   loss 0.201\n[13] step 800   loss 0.135\nEpoch 13  valid WER: 22.273%\n[14] step 0   loss 0.128\n[14] step 100   loss 0.147\n[14] step 200   loss 0.166\n[14] step 300   loss 0.155\n[14] step 400   loss 0.126\n[14] step 500   loss 0.126\n[14] step 600   loss 0.136\n[14] step 700   loss 0.150\n[14] step 800   loss 0.123\nEpoch 14  valid WER: 21.763%\n[15] step 0   loss 0.119\n[15] step 100   loss 0.104\n[15] step 200   loss 0.116\n[15] step 300   loss 0.086\n[15] step 400   loss 0.113\n[15] step 500   loss 0.113\n[15] step 600   loss 0.116\n[15] step 700   loss 0.120\n[15] step 800   loss 0.112\nEpoch 15  valid WER: 20.968%\n[16] step 0   loss 0.101\n[16] step 100   loss 0.100\n[16] step 200   loss 0.089\n[16] step 300   loss 0.095\n[16] step 400   loss 0.076\n[16] step 500   loss 0.095\n[16] step 600   loss 0.085\n[16] step 700   loss 0.097\n[16] step 800   loss 0.115\nEpoch 16  valid WER: 20.557%\n[17] step 0   loss 0.067\n[17] step 100   loss 0.101\n[17] step 200   loss 0.097\n[17] step 300   loss 0.087\n[17] step 400   loss 0.098\n[17] step 500   loss 0.093\n[17] step 600   loss 0.075\n[17] step 700   loss 0.095\n[17] step 800   loss 0.123\nEpoch 17  valid WER: 20.218%\n[18] step 0   loss 0.089\n[18] step 100   loss 0.069\n[18] step 200   loss 0.076\n[18] step 300   loss 0.090\n[18] step 400   loss 0.109\n[18] step 500   loss 0.076\n[18] step 600   loss 0.096\n[18] step 700   loss 0.075\n[18] step 800   loss 0.076\nEpoch 18  valid WER: 19.927%\n[19] step 0   loss 0.098\n[19] step 100   loss 0.090\n[19] step 200   loss 0.077\n[19] step 300   loss 0.082\n[19] step 400   loss 0.093\n[19] step 500   loss 0.090\n[19] step 600   loss 0.112\n[19] step 700   loss 0.056\n[19] step 800   loss 0.084\nEpoch 19  valid WER: 19.975%\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Post-training Evaluation\n\nAfter the network has finished learning we need an objective scoreboard.\nThis block switches the model to inference mode, runs it once over the\ntest_loader, and prints the two ASR staples — Word-Error-Rate (WER)\nand Character-Error-Rate (CER). We use greedy decoding (arg-max per\nframe, then CTC collapse) because it is fast and gives a lower-bound on the\nmodel’s true performance.","metadata":{}},{"cell_type":"code","source":"model.eval(); hyp, ref = [], []\nwith torch.no_grad():\n    for x, y, in_len, tar_len in test_loader:\n        x = x.to(device)\n        lp = model(x)                    # (B, T, V)\n        ids = lp.argmax(-1)              # greedy\n        for i, l in enumerate(in_len):\n            hyp.append(decode([k for k,_ in groupby(ids[i][:l].cpu().tolist()) if k!=0]))\n        split = torch.split(y, tar_len)\n        ref.extend([decode(t.tolist()) for t in split])\n    val_wer = wer(ref, hyp)\n    val_cer = cer(ref, hyp)\n    print(f\"WER: {val_wer:.3%}\")\n    print(f\"CER: {val_cer:.3%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:43:10.182929Z","iopub.execute_input":"2025-05-28T01:43:10.183710Z","iopub.status.idle":"2025-05-28T01:44:59.238380Z","shell.execute_reply.started":"2025-05-28T01:43:10.183672Z","shell.execute_reply":"2025-05-28T01:44:59.237720Z"}},"outputs":[{"name":"stdout","text":"WER: 19.975%\nCER: 6.395%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"* The network gets most letters right, so character accuracy is good.\n\n* But a single letter slip can break an entire word, inflating WER.\n\n* Word-boundary mistakes (extra or missing spaces) also hurt WER but not CER.\n\nHow good is ~20 % WER on train-clean-100 -> test-clean?\n\n| Setup (literature)                                            | WER on *test-clean* |\n| ------------------------------------------------------------- | ------------------- |\n| Shallow CRNN (≈23 M params) trained **from scratch** on 100 h | **18–22 %**         |\n| Same network + **2-gram KenLM beam search**                   | 14–17 %             |\n| Pre-trained *wav2vec 2.0* base → fine-tuned on 100 h          | 4–6 %               |\n| OpenAI *Whisper-tiny.en* fine-tune (≈40 M params)             | ≈ 4 %               |\n","metadata":{}},{"cell_type":"code","source":"print(\"Target example :\", ref[0])\nprint(\"Prediction :\", hyp[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T02:18:18.682579Z","iopub.execute_input":"2025-05-28T02:18:18.682862Z","iopub.status.idle":"2025-05-28T02:18:18.686985Z","shell.execute_reply.started":"2025-05-28T02:18:18.682841Z","shell.execute_reply":"2025-05-28T02:18:18.686218Z"}},"outputs":[{"name":"stdout","text":"Target example : he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\nPrediction : he hoped there would be stool for dinner turnips and carots and bruised potatos and fat muttin pieces to be ladled out and thick peppered flower fattens sauce\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"* The core content is intact; every noun but one is recognisable, which aligns with the low CER (6 %).\n\n* Most mistakes are homophones or single-letter drops—classic symptoms of a character CTC model decoded without linguistic context.\n\n* A small language model or even the simple spell-checker post-filter already implemented would likely fix flour/flower, stew/stool, carrots, potatoes, and fattened, reducing WER by several points.\n\nThis single sentence illustrates how the quantitative numbers (≈ 20 % WER, 6 % CER) translate into qualitative perception: the transcript is easily understandable, yet word-level polish is needed for production-grade accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Post-correction\n\nAfter CTC decoding the acoustic model still produces small, but WER-inflating, surface errors—single-letter drops (potatos), homophones (flour -> flower), or wrong inflections (fattened -> fattens). Rather than retrain the network we pass the raw transcript through a lightweight dictionary-based spell-checker (pyspellchecker). \n\nFor every token that is not in the vocabulary the module picks the most probable alternative within one edit distance, exploiting corpus word-frequency statistics. This zero-cost CPU step typically removes 1–3% of WER on LibriSpeech-scale data, turning an already intelligible output into a clean, correctly spelled sentence.","metadata":{}},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker(distance=2)          # edit-distance ≤1  (fast; good when CER≈6 %)\n\ndef post_correct(sentence: str) -> str:\n    \"\"\"\n    Replace each OOV token with SpellChecker's best suggestion.\n    Keeps punctuation and numbers untouched.\n    \"\"\"\n    corrected = []\n    for token in sentence.split():\n        # skip tokens that are in dictionary OR contain non-letters\n        if token.lower() in spell or not token.isalpha():\n            corrected.append(token)\n        else:\n            corrected.append(spell.correction(token) or token)  # fallback: keep as is\n    return \" \".join(corrected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:58:07.411427Z","iopub.execute_input":"2025-05-28T01:58:07.411695Z","iopub.status.idle":"2025-05-28T01:58:07.528931Z","shell.execute_reply.started":"2025-05-28T01:58:07.411666Z","shell.execute_reply":"2025-05-28T01:58:07.528213Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"hyps_raw, refs = [], []\n\nwith torch.no_grad():\n    for feats, labels, in_len, tar_len in test_loader:\n        feats = feats.to(device, non_blocking=True)\n        logp  = model(feats)\n        pred  = logp.argmax(-1).cpu()\n\n        for i, T in enumerate(in_len):\n            ids = [k for k, _ in groupby(pred[i][:T].tolist()) if k != 0]\n            txt = decode(ids)\n            hyps_raw.append(txt)\n\n        split = torch.split(labels, tar_len)\n        refs.extend(decode(t.tolist()) for t in split)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T01:58:10.563844Z","iopub.execute_input":"2025-05-28T01:58:10.564321Z","iopub.status.idle":"2025-05-28T01:59:59.224340Z","shell.execute_reply.started":"2025-05-28T01:58:10.564285Z","shell.execute_reply":"2025-05-28T01:59:59.223642Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Apply post-correction\nhyps_spell = [post_correct(h) for h in hyps_raw]\n\n# Compute metrics – before / after spell-check\nwer_raw  = wer(refs, hyps_raw)\ncer_raw  = cer(refs, hyps_raw)\nwer_post = wer(refs, hyps_spell)\ncer_post = cer(refs, hyps_spell)\n\nprint(f\"Raw  →  WER {wer_raw:.2%}   |  CER {cer_raw:.2%}\")\nprint(f\"Spell-corrected  →  WER {wer_post:.2%}   |  CER {cer_post:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T02:00:03.415548Z","iopub.execute_input":"2025-05-28T02:00:03.416219Z","iopub.status.idle":"2025-05-28T02:13:09.264966Z","shell.execute_reply.started":"2025-05-28T02:00:03.416192Z","shell.execute_reply":"2025-05-28T02:13:09.264095Z"}},"outputs":[{"name":"stdout","text":"Raw  →  WER 19.97%   |  CER 6.40%\nSpell-corrected  →  WER 16.91%   |  CER 6.49%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(\"Target example :\", refs[0])\nprint(\"Prediction :\", hyps_spell[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T02:17:56.875224Z","iopub.execute_input":"2025-05-28T02:17:56.875815Z","iopub.status.idle":"2025-05-28T02:17:56.879625Z","shell.execute_reply.started":"2025-05-28T02:17:56.875794Z","shell.execute_reply":"2025-05-28T02:17:56.878990Z"}},"outputs":[{"name":"stdout","text":"Target example : he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\nPrediction : he hoped there would be stool for dinner turnips and carrots and bruised potato and fat mutton pieces to be ladled out and thick peppered flower fattens sauce\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Transcription\n\nAfter training we need a convenient, reusable function that turns an arbitrary .wav clip into plain text. The transcribe routine below wraps the whole inference pipeline in five concise steps:\n\n1. Load and preprocess The waveform is read with torchaudio.load, resampled (if needed) and converted to a log-Mel spectrogram by re-using the preprocess function.\n\n2. Batchify & move to GPU/CPU A dummy batch dimension is added so the tensor shape matches what the network expects, and the tensor is copied to the chosen device.\n\n3. Forward pass (no gradients) Inside torch.no_grad() the model produces log-probabilities over the vocabulary for every time frame.\n\n4. Greedy CTC decoding argmax selects the highest-probability token per frame; consecutive duplicates and blank tokens are collapsed, then mapped back to characters with decode.\n\n5. Optional spell-checking If spellcheck=True, the raw string is run through the post_correct spell-checker to fix common single-letter or homophone errors, improving readability and WER by a few points.\n\nThe function returns the final transcript string and can be called on any short (≈10 s) WAV file in a single line","metadata":{}},{"cell_type":"code","source":"# Single-file transcription routine\ndef transcribe(wav_path: str, spellcheck: bool = True) -> str:\n    \"\"\"Return the (optionally spell-corrected) transcription of wav_path.\"\"\"\n    wav, sr = torchaudio.load(wav_path)\n    feat = preprocess(wav, sr, augment=False)        # (T, F)\n    feat = feat.unsqueeze(0).to(device)              # (1, T, F)\n\n    with torch.no_grad():\n        logits = model(feat)                         # (1, T, V)\n\n    ids = logits.argmax(-1)[0].cpu().tolist()   # (T,)\n    ids = [k for k, _ in groupby(ids) if k != 0]\n    text = decode(ids)\n        \n    if spellcheck:\n        text = post_correct(text)\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T02:38:07.683581Z","iopub.execute_input":"2025-05-28T02:38:07.683904Z","iopub.status.idle":"2025-05-28T02:38:07.689562Z","shell.execute_reply.started":"2025-05-28T02:38:07.683885Z","shell.execute_reply":"2025-05-28T02:38:07.688990Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"To test how well our purely English-trained model copes with foreign accents, we fed it a 10-second extract from a speech by François Hollande, former President of France, a clear non-native English speaker. Even though LibriSpeech contains only native (North-American) readers, the model still captures the overall structure of the sentence.","metadata":{}},{"cell_type":"code","source":"wav_file = \"/kaggle/input/hollande/francois_hollande.wav\"\nprint(\"Transcription:\\n\", transcribe(wav_file))\nprint(\"Transcription (no post-correction):\\n\", transcribe(wav_file, False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T03:00:44.070708Z","iopub.execute_input":"2025-05-28T03:00:44.071004Z","iopub.status.idle":"2025-05-28T03:00:44.320412Z","shell.execute_reply.started":"2025-05-28T03:00:44.070984Z","shell.execute_reply":"2025-05-28T03:00:44.319738Z"}},"outputs":[{"name":"stdout","text":"Transcription:\n repro all you because you gon't be do what we wale to do\nTranscription (no post-correction):\n reprod  all you becauese you gon't be do whatk we wale to do\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"The lightweight post-correction layer fixes many of these letter-level slips, yielding a far more legible transcription with no extra acoustic training.\n\nThis small experiment highlights two points:\n\n* The CRNN-CTC network generalises beyond native accents surprisingly well, thanks to its character-level target space and SpecAugment robustness.\n\n* A cheap text-only spell-checker can further clean up accent-induced spelling noise, closing much of the gap to native-speaker performance without re-training the model on accented data.","metadata":{}}]}